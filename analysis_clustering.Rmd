---
title: 'Hunter College Data: Clustering Analysis'
author: "Krista DeStasio"
date: "11/06/2018"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: no
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document:
    toc: yes
    toc_depth: '3'
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Clear workspace
rm(list = ls()) 

# Set paths and working directory
working_dir = '~/Dropbox/collaborations/hunter_college/hc_analysis/'
path_datafile = paste0(working_dir, '/data/N=844_FINAL_Traditional metrics_Trial Level metrics_questionnaires (n=837)_6.16.18.xlsx')
setwd(working_dir)

# Install and load required packages
list.of.packages <- c('janitor', 'kernlab', 'dplyr', 'ggplot2', 'tidyr', 'knitr', 'cluster', 'mclust', 'flexclust', 'factoextra', 'fpc', 'NbClust')
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])] 
if (length(new.packages)) install.packages(new.packages, repos = "http://cran.us.r-project.org")
lapply(list.of.packages, library, character.only = TRUE)

# Knit options
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.path = 'Figs/', echo = FALSE, eval = TRUE)
```

# Background information  

*Tracy writes:* "As I see it our first goal is testing whether distinct attention bias grouping emerge from the dot probe and questionnaire data using classification algorithms. Once these groups are identified, we can look at whether these are treatment-relevant subgroups (i.e., ABMT training is more effective for one group versus the other)."  

"examine the conditions under which ABMT efficacy is boosted or disrupted, and to identify individual differences that impact training gains"

*A search for moderators that machine learning may be sensitive enough to detect.*  

# Analysis plan

All analyses will be conducted using R (R Core Team, 2018). 


## Aim 1 
**Aim 1** is to identify whether meaningful subgroups emerge from the anxiety dot probe. To conduct this exploratory analysis, we will use several clustering algorithms. Clustering algorithms are designed to group data based on their similarity or dissimilarity (e.g. distance in euclidian space). Prior to clustering, dimension reduction is conducted via principal components analysis to decresase overfitting and to make the models more inerpretable (Zha, et al., 2002; Ding & He, 2004). The final solution will be selected based on visual inspection of the cluster outcomes, and based on the SSE for each solution. Outcomes of a spectral clustering analysis (connectivity), a k-means cluster analysis (compactness), and a hierarchical clustering analysis are compared to identify subgroups within the Anxiety Dot Probe measures. These approaches are well suited to numeric data. K-means is a partition based clustering algorithm that groups data points by their euclidian distance from the cluster's centroid, the mean of that cluster's objects (Nerurkar et al., 2018).  Spectral clustering also uses euclidian distance to group data points, but by representing the data as a similarity graph in which data poionts are nodes and the distances between them are weighted edges (Von Luxburg, 2007). Both k-means and spectral clustering require a priori specification of the number of clusters. Hierarchical clustering, in contrast, creates multiple levels of clusters and subclusters (Cichosz, 2015). As such, hierarchical clustering does not require a priori specification of number of clusters.  

This analysis is done with the eventual goal of predicting treatment trajectories in an unrelated sample of Attention Bias Modification Training (ABMT) recipients. In that analysis, we will test whether cluster membership can predict the efficacy of ABMT (toward or away from threat) as measured by a post-ABMT re-application of the anxiety dot-probe.  

## Aim 2
**Aim 2** is to test the hypothesis that bias toward threat during the anxiety dot probe correlates with greater trait anxiety as measured by State-Trait Anxiety Inventory (STAI) scores. To test this aim, I will run two separate analyses. The first is a linear regression with three threat bias scores entered as covariates: (1) threat bias ("average RTs for neutral probes in TN trials minus RTs for angry probes in TN trials"), (2) vigilance ("average RTs for neutral probes in NN trials minus RTs for angry probes in TN trials"), and (3) disengagement ("average RTs for neutral probes on TN trials minus RTs for neutral probes on NN trials").

`aov(STAI_anxiety ~ threat_bias*vigilance*disengagement)`

Second, I will test whether group membership, derived from the Aim 1 analysis, predicts STAI trait anxiety. Group membership will be entered as an independent variable into a between-subjects ANOVA with STAI trait anxiety scores as the dependent measure of interest. 

`aov(STAI_anxiety ~ cluster_groups + Error(subject/cluster_groups), data=full_dataset)`
 

## Data set
Dot Probe measures to be explored as features in the clustering analysis are:  

- `rt_neutral_nt`: Average RT on all neutral trials  
- `rt_threat_nt`: Average RT on all threat trials  
- `rt_baseline`: Average RT on baseline trials where two neutral faces appear and there is no competition for attention between threat and neutral faces; These trials appear randomly throughout the task  
- `mean_pos`: mean of positive trial-level threat bias scores  
- `mean_neg`: mean of negative trial-level threat bias scores      
- `peak_pos`: highest positive trial-level threat bias score
- `peak_neg`: highest negative trial-level threat bias score           

Variables for the regression analysis (Aim 2):   

- `threatbias_ERLAB`  
- `vigilance_ERLAB`  
- `disengage_ERLAB`  
- `stai_trait_totalscore`

Variables that will not be included in the analysis as they describe information already available from the other measures:

- `rt_outliers`: # of trials considered to be outliers based on the following criteria (RTs faster than 150 ms or slower than 2000 ms; any trial RT that was +/- 3 SD from the personâ€™s mean RT for that trial type)
- `variability`: absolute value of the distance across all trial-level threat bias scores / number of pairs

## Note
There is likely value in including additional features (e.g. age, questionnaire scores). Discuss with Tracy's group whehter they want to include those measures or restrict the clustering factors to dot probe measures.  

# Analysis and Results

```{r my functions}
# Run k-means clustering with variable numbers of centers, plot the results, and return the within cluster sum of squares
myfun_multiplot_km <- function(df, centers_to_try, id){
    # Initialize total within sum of squares: wss
    set.seed(50)
    wss <- 0
    
    # For 1 to 4 cluster centers
    for (i in 1:centers_to_try) {
      km_out <- kmeans(df, centers = i, nstart = 20)
      # Save total within sum of squares to wss variable
      wss[i] <- km_out$tot.withinss
      
      # Plot clusters
      plot(df, col = km_out$cluster, 
           main = paste(id, 'Data, Total within-cluster sum of squares \n', round(km_out$tot.withinss, 2)), 
           xlab = "", ylab = "")
      assign(paste0('wss_km_', id), wss, envir = globalenv())
    }
}

# Create scree plots of within sums of squares by number of clusters
myfun_myscreeplot <- function(wss, clus_range, title){
    plot(clus_range, wss, type = "b", 
         xlab = "Number of Clusters", 
         ylab = "Within groups sum of squares", 
         main = title,
         ylim = c(0, 7000))
}

# Function to plot means by variable and cluster memberships
myfun_plotmeansbyclus <- function(clusters_to_plot, title){
    dataframe <-  cbind(data_matrix[,1], clusters_to_plot, data_matrix_aim1) %>% 
    gather(., variable, measurement, rt_neutral_nt:variability)
    dataframe$clusters <- as.factor(dataframe$clusters_to_plot)
    
    ggplot(dataframe, aes(x = variable, y = measurement, fill = clusters)) + 
        geom_point(alpha = 0.2,  pch = 21, position = position_jitterdodge()) +
        geom_boxplot(alpha = 0.4) +
        theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
        ggtitle(title) 
}

# Function to get raw score means based on cluster membership
myfun_getmeans <- function(cluster_solution, title){
    kable(aggregate(data_matrix_aim1, 
                    by = list(cluster = cluster_solution), 
                    mean), 
          caption = title,
          digits = 2)
}

# Run spectral clustering with variable numbers of centers, plot the results, and return the within cluser sum of squares
myfun_multiplot_specc <- function(df, centers_to_try, id){
    # Initialize total within sum of squares: wss
    set.seed(50)
    wss <- 0
    
    # For 1 to 4 cluster centers
    for (i in 2:centers_to_try) {
      specc_out <- specc(df, centers = i) 
      # Save total within sum of squares to wss variable
      wss[i] <- withinss(specc_out)
      
      # Plot clusters
    plot(df, 
     col = specc_out, pch = 4, 
     main = paste('Spectral Clustering Results with \n', i, 'Clusters Mapped on \n', id, 'Data'))  
      assign(paste0('wss_specc_', id), wss, envir = globalenv())
    }
}
```

## Data cleaning

- Use `janitor` to make the dataset names a consistent format.  
- Subset the data into the set that will be used for the cluster analysis (all variables except STAI score)  

```{r, include=FALSE}
# Read in the data
data_frame <- clean_names(readxl::read_excel(path_datafile)) # When ready to run, set to path_datafile

# Subset dot probe measures
data_matrix <- data_frame[,c(1, 3:14, 22)]
data_matrix_aim1 <- subset(data_matrix, 
                           select = -c(subject_id,
                                       threatbias_erlab, 
                                       vigilance_erlab, 
                                       disengage_erlab, 
                                       stai_trait_totalscore,
                                       rt_outliers))
data_matrix_aim2 <- subset(data_matrix, 
                           select = c(subject_id,
                                      threatbias_erlab, 
                                      vigilance_erlab, 
                                      disengage_erlab, 
                                      stai_trait_totalscore))
# Any missing data?
table(is.na(data_matrix_aim1)) # There are no missing data
```

## Pre-processing 
### Outliers and missingness
Outliers were already removed from the data by our collaborators based on the percentage of trials that were answered correctly. Participants with an accuracy of .80 or greater are included in the dataset. There should be no initial missing data for the dot probe measures as the included metrics can be computed for anyone who completed the task. Missing survey data will be handled in the analysis for Aim 2.  

```{r, include=FALSE}
# Make sure there are no outliers
OutVals = boxplot(data_matrix_aim1, plot = FALSE)$out
which(data_matrix_aim1 %in% OutVals) # No outliers identified
```

### Zero and near zero variance
All variables were checked for zero or near zero variance and no issues were found.  

```{r Calculate variance metrics, include=FALSE}
# http://topepo.github.io/caret/pre-processing.html
# Frequency ratio & % of unique values
caret::nearZeroVar(data_matrix_aim1, saveMetrics = TRUE) # none of the variables have near zero variance 
```

### Linear dependencies

The dataset was checked for variables that are linear combinations of other variables with the intention to remove those indicated. None of the vairables included in the cluster analysis are colinear.  

```{r Linear dependency, include=FALSE}
# Identify linear combinations
caret::findLinearCombos(data_matrix_aim1) # None of the variables are linear combinations of others
```

### Number of observations & features
```{r}
num_obs <- dim(data_matrix_aim1) # There are 844 observations of 9 features
```

There are `r num_obs[1]` observations across `r num_obs[2]` features.  
```{r}
rm(num_obs)
```

### Standardization/Scaling

Clustering is sensitive to differences in the measurement scales of data. To check whether any of the features differ in scale, we look at the whether the means and standard deviations vary across the features.  

#### Means and standard deviations
```{r check initial means and sds}
raw_means <- colMeans(data_matrix_aim1) # means
raw_sds <- apply(data_matrix_aim1, 2, sd) # standard deviation for each column
```

Since the means and standard deviations do vary across features, standardize them by mean centering Using the `scale()` function, variables are scaled to mean = 0, sd = 1.   

```{r mean centering}
scaled_data <- scale(data_matrix_aim1) # scale the data
scaled_data_id <- cbind(data_matrix[,1], scaled_data)

# Check that the data are scaled
scaled_means <- round(colMeans(scaled_data), 2)
scaled_sds <- apply(scaled_data, 2, sd)
kable(rbind(raw_means, raw_sds, scaled_means, scaled_sds), digits = 2)
```

## Dimensionality Reduction, Principal Components Analysis  
**Purpose: prevent overfitting of the data and improve interpretability**

Principal Components Analysis (PCA) is a dimensionality reduction technique that can be used as a precursor to k-means clustering (Zha, et al., 2002; Ding & He, 2004). PCA projects high dimensional data onto a lower dimensional sub-space and rotates data to mazimize variance in the new axes. The components are then listed in order of  decreasing variance (e.g. component 1 has the most variation in the data, component 2 is orthogonal to component 1 and has the most remaining variation, and so on). PCA is an unsuprevised technique that can be used as in this case when we do not have hypotheses about the distribution of varaince across features.   

```{r PCA}
set.seed(50)

plot(data_matrix_aim1)
pca_scaled <- prcomp(x = scaled_data)
pca_summary <- summary(pca_scaled)
```

### Visualization
#### Biplot
```{r}
# Biplot
biplot(pca_scaled)
```

#### Plots of variance by number of components
```{r}
# Get the proportion of variance
pca_var <- pca_scaled$sdev^2
prop_var_each <- pca_var / sum(pca_var)

par(mfrow = c(1,2)) 
# Plot the variance explained for each principal component
plot(prop_var_each, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0,1), type = "b")

# Plot cumulative proportion of variance explained
plot(cumsum(prop_var_each), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0,1), type = "b")
```

```{r}
data_pca2components <- pca_scaled$x[,1:2]
data_pca3components <- pca_scaled$x[,1:3]
data_pca4components <- pca_scaled$x[,1:4]
```

### Conclusions from PCA

```{r}
kable(pca_summary$importance, digits = 2, 
      caption = 'PCA Summary Table')
```

2 or 3 components are sufficient to account for a substantial portion of variance.  

## K-means cluster analysis
K-means is a centroid based clustering approach that groups data points around a central point to minimize within group distance and maximize between group distance. To help determine the number of clusters to use, k-means is run with 1 through 4 clusters. The total within-cluster sum of squares is recorded for each iteration with a different cluster number. The results are displayed as scatterplots and as a scree plot. The elbow in the scree plot will guide the decision of how many clusters to use.  

### Plots

Without PCA: 1-4 k-means clusters
```{r}
par(mfrow = c(2,2)) 
myfun_multiplot_km(scaled_data, 4, 'Scaled')
```

#### With 2 Principal Components and 1-4 k-means clusters
```{r run kmeans PCA[1:2], warning=FALSE}
par(mfrow = c(2,2)) 
myfun_multiplot_km(data_pca2components, 4, 'PCA2')
```

#### With 3 Principal Components and 1-4 k-means clusters
```{r run kmeans PCA[1:3], warning=FALSE}
par(mfrow = c(2,2)) 
myfun_multiplot_km(data_pca3components, 4, 'PCA3')
```

#### With 4 Principal Components and 1-4 k-means clusters
```{r run kmeans PCA[1:4], warning=FALSE}
par(mfrow = c(2,2)) 
myfun_multiplot_km(data_pca4components, 4, 'PCA4')
```

#### Within Cluster Sum of Squares

```{r kmeans within ss}
par(mfrow = c(1,2)) 

myfun_myscreeplot(wss_km_Scaled, 1:4,'K-Means Within Cluster Sum of Squares for \n1 Through 4 Clusters: \n with Scaled Data')
myfun_myscreeplot(wss_km_PCA2, 1:4, 'K-Means Within Cluster Sum of Squares for \n1 Through 4 Clusters: \n with PCA2 Data')
myfun_myscreeplot(wss_km_PCA3, 1:4, 'K-Means Within Cluster Sum of Squares for \n1 Through 4 Clusters: \n with PCA3 Data')
myfun_myscreeplot(wss_km_PCA4, 1:4, 'K-Means Within Cluster Sum of Squares for \n1 Through 4 Clusters: \n with PCA4 Data')
```

### K-means Cluster Summary
#### Plots of variables by cluster

```{r examine kmeans results}
for (i in 2:3) {
set.seed(50)    
    # K-means clustering
    km_res <- eclust(data_pca2components, 
                     FUNcluster = 'kmeans', 
                     hc_metric = 'euclidean',
                     k = i, nstart = 25, graph = TRUE)
    
    # Visualization of K-means clustering
   clus_plot <-  fviz_cluster(km_res, geom = 'point', ellipse.type = 'norm', palette = 'jco',
                      print.summary = FALSE,
                      ggtheme = theme_minimal(), 
                      main = paste0("Cluster Plot of K-means With ", i, " Clusters"))
    
    # Silhouette Plot
   sil_plot <- fviz_silhouette(km_res, palette = "jco",
                         print.summary = FALSE) + 
             theme_minimal() + 
             ggtitle(paste0("Silhouette Plot of K-means With ", i, " Clusters"))
    
    # Silhouette information
    silinfo <- km_res$silinfo
    
    df1 <- as.data.frame(cbind(paste0('k-means: ', i, ' clusters'),
        c(1:i), 
        km_res$size, 
        silinfo$clus.avg.widths)) #Average silhouette width of each cluster
    colnames(df1) <- c('Model', 'Cluster', 'Cluster size', 'Average silhouette width')
   
    
    # Statistics for k-means clustering
    km_stats <- cluster.stats(dist(data_pca2components),  km_res$cluster)
    # Silhouette width of observation
    sil <- km_res$silinfo$widths[, 1:3]
    # Objects with negative silhouette
    neg_sil_index <- which(sil[, 'sil_width'] < 0)
    
    df2 <- as.data.frame(cbind(paste0('k-means: ', i, ' clusters'),
       silinfo$avg.width, 
       km_stats$average.between,
       km_stats$average.within,
       length(neg_sil_index))) #Average silhouette width of each cluster
    colnames(df2) <- c('Model', 'Average silhouette coefficient', 'Average distance between clusters', 'Average distance within clusters', '# observations with negative silhouette')

    
    # save objects and clean environment
    assign(paste0("km_k", i, '_res'), km_res, envir = globalenv())
    assign(paste0("km_k", i, '_stats'), km_stats, envir = globalenv())
    assign(paste0("km_k", i, '_df_clusstats'), df1, envir = globalenv())
    assign(paste0("km_k", i, '_df_fit'), df2, envir = globalenv())
    assign(paste0("km_k", i, '_clusplot'), clus_plot, envir = globalenv())
    assign(paste0("km_k", i, '_silplot'), sil_plot, envir = globalenv())
    
    rm(km_res, km_stats, sil, silinfo, df1, df2, clus_plot, sil_plot)
}
```

```{r kmeans plot var means by cluster}
par(mfrow = c(1,2)) 
myfun_plotmeansbyclus(km_k2_res$cluster, 'K-Means PCA2 Clustering Solution: 2 Clusters, Raw Variable Scores')
myfun_plotmeansbyclus(km_k3_res$cluster, 'K-Means PCA2 Clustering Solution: 2 Clusters, Raw Variable Scores')
```

#### Summary Statistics
```{r k-means cluster means and standard deviations}
(variable_means_km2 <- myfun_getmeans(km_k2_res$cluster, "K-means 2 Cluster Solution Developed on Data Reduced to 2 Principal Components: Raw Score Means by Cluster"))

(variable_means_km3 <- myfun_getmeans(km_k3_res$cluster, "K-means 3 Cluster Solution Developed on Data Reduced to 2 Principal Components: Raw Score Means by Cluster"))
```

```{r}
table(km_k2_res$cluster)
table(km_k3_res$cluster)
table(km_k2_res$cluster, km_k3_res$cluster)
```

```{r save the model as a kcca object, warning=FALSE}
kmeans_kcca <- as.kcca(km_k2_res, data_pca2components)
```

#### Write-up

Based on the scree plot and scatterplots, a model developed from data reduced to **2 PCA components** and clustered with **2 or 3 k-means centroids** appears to best reduce the within group sum of squares without much additional benefit for including additional centroids.    

## Spectral Clustering

Spectral clustering presents an alternative to k-means that uses data points as nodes and the distance between them as edges in a weighted graph (von Luxburg, 2007). This method is less susceptable to local minima and to non-gaussian data density (Ng, et al., 2001). For our analyses, we used normalized spectral clustering (Luxburg et al., 2005) using the kernlab (Karatzoglou et al., 2004) package for R.  

### Without PCA
```{r Run the spectral clustering analysis}
par(mfrow = c(1,2)) 
myfun_multiplot_specc(scaled_data, 4, 'Scaled')
```

### With 2 Principal Components
```{r spectral clustering analysis with 2PCs}
par(mfrow = c(1,2)) 
myfun_multiplot_specc(data_pca2components, 4, 'PCA2')
```

### With 3 Principal Components
```{r spectral clustering analysis with 4PCs}
par(mfrow = c(1,2)) 
myfun_multiplot_specc(data_pca3components, 4, 'PCA3')
```

### With 4 Principal Components

```{r spectral clustering analysis with 3PCs}
par(mfrow = c(1,2)) 
myfun_multiplot_specc(data_pca4components, 4, 'PCA4')
```

```{r run the models that look best}
spec_2clust <- specc(scaled_data, centers = 2) 
spec_2clust_pca2 <- specc(data_pca2components, centers = 2) 
```

### Within Sum of Squares
```{r spec clust within ss}
par(mfrow = c(2,2)) 
myfun_myscreeplot(wss_specc_Scaled[2:4], 2:4, 'Spectral Clustering Within Cluster Sum of Squares for \n2 Through 4 Clusters: \n with Scaled Data')
myfun_myscreeplot(wss_specc_PCA2[2:4], 2:4, 'Spectral Clustering Within Cluster Sum of Squares for \n2 Through 4 Clusters: \n with PCA2 Data')
myfun_myscreeplot(wss_specc_PCA3[2:4], 2:4, 'Spectral Clustering Within Cluster Sum of Squares for \n2 Through 4 Clusters: \n with PCA3 Data')
myfun_myscreeplot(wss_specc_PCA4[2:4], 2:4, 'Spectral Clustering Within Cluster Sum of Squares for \n2 Through 4 Clusters: \n with PCA4 Data')
```

### Spectral Clustering Summary 
#### Plots of variables by cluster
```{r}
myfun_plotmeansbyclus(spec_2clust, 'Spectral Clustering Solution: \n2 Clusters Scaled Data, Raw Variable Scores')
myfun_plotmeansbyclus(spec_2clust_pca2, 'Spectral Clustering Solution: \n2 Clusters from 2 Component PCA Data, Raw Variable Scores')
```

#### Summary statistics
```{r spectral clustering solution means}
myfun_getmeans(spec_2clust_pca2, "Spectral Clustering 2 Cluster Solution Developed on Scaled Data: Raw Score Means by Cluster")
myfun_getmeans(spec_2clust, "Spectral Clustering 2 Cluster Solution Developed on Data Reduced to 2 Principal Components: Raw Score Means by Cluster")
```

```{r spec clust tables of group membership}
table(spec_2clust)
table(spec_2clust_pca2)
table(spec_2clust, spec_2clust_pca2)
```

#### Write-up

None of the spectral clustering solutions is particularly satisfying. A model developed from data reduced to **2 PCA components** and clustered with **2 centroids** seems to provide the clearest clusters. Compared to the 2 cluster solution developed on the scaled but non-reduced data, it had lower within cluster sum of squares 

## Hierarchical clustering

Used when the number of clusters is not known ahead of time. There's top-down and bottom-up clustering. I will be using bottom-up clustering, which starts by assigning each point to it's own cluster, then iteratively combines them into fewer clusters based on distance between them until there's only 1 cluster. There are multiple ways to calculate the distance between observations. I will use euclidean distance. 

Create the first hierarchical clustering model.  

To determine the distance between clusters, one of 4 methods can be used:  
 (1) *Complete* - measures pairwaise similarity between clusters and uses the largest.  
 (2) *Single* - same as above, but uses smallest of similarities.  
 (3) *Average* - same as above, but uses average of similarities.  
 (4) *Centroid* - centroid of cluster 1 is calculated, centroid of cluster 2 is calculated and the distance between the 2 centroids is used.  
 
Complete and average tend to produce more balanced trees and are the most common and so are the methods applied here.  

```{r hierarchical clustering models}
methods <- c("ward.D2", "complete", "average")

for (method in methods) {
    set.seed(50)
    for (i in 2:3) {
        cat(paste0("\n", method, " method: ", i, " clusters\n"))
        # Hierarchical clustering
        hc_res <- eclust(data_pca2components, 
                         FUNcluster = "hclust", 
                         k = i, 
                         hc_metric = "euclidean", 
                         hc_method = method, graph = TRUE)

        # Visualize dendrograms
        dendr_plot <- fviz_dend(hc_res, show_labels = FALSE, palette = "jco",
                       rect = TRUE,
                       print.summary = FALSE,
                       as.ggplot = TRUE,
                       main = paste0("Cluster Plot of Hierarchical Clustering With ", method, " Method and ", i, " Centers"))

        # Visualize clusters
        clus_plot <- fviz_cluster(hc_res, geom = 'point', ellipse.type = 'norm', palette = 'jco',
                          print.summary = FALSE,
                          ggtheme = theme_minimal(),
                          main = paste0("Cluster Plot of Hierarchical Clustering With ", method, " Method and ", i, " Centers"))

        # Silhouette Plot
        sil_plot <- fviz_silhouette(hc_res, palette = "jco",
                             print.summary = FALSE) +
                 theme_minimal() +
                 ggtitle(paste0("Cluster Plot of Hierarchical Clustering With ", method, " Method and ", i, " Centers"))

        # Silhouette information
        silinfo <- hc_res$silinfo
        df1 <- as.data.frame(cbind(
            paste0('Hierarchical ', method, ' : ', i, ' clusters'),
            c(1:i), 
            hc_res$size, 
            silinfo$clus.avg.widths)) #Average silhouette width of each cluster
        colnames(df1) <- c('Model', 'Cluster', 'Cluster size', 'Average silhouette width')
        
        # Statistics for hierarchical clustering
        hc_stats <- cluster.stats(dist(data_pca2components),  hc_res$cluster)
        # Silhouette width of observation
        sil <- hc_res$silinfo$widths[, 1:3]
        # Objects with negative silhouette
        neg_sil_index <- which(sil[, 'sil_width'] < 0)
        
        df2 <- as.data.frame(cbind(
            paste0('Hierarchical ', method, ' : ', i, ' clusters'),
            silinfo$avg.width, 
            hc_stats$average.between,
            hc_stats$average.within,
            length(neg_sil_index))) 
        colnames(df2) <- c('Model', 'Average silhouette coefficient', 'Average distance between clusters', 'Average distance within clusters', '# observations with negative silhouette')
        
        # save objects and clean environment
        assign(paste0("hc_", method, '_k', i), hc_res, envir = globalenv())
        assign(paste0("hc_", method, '_k', i, '_stats'), hc_stats, envir = globalenv())
        assign(paste0('hc_', method, '_k', i, '_clusstats'), df1, envir = globalenv())
        assign(paste0('hc_', method, '_k', i, '_fit'), df2, envir = globalenv())
        assign(paste0('hc_', method, '_k', i, '_clusplot'), clus_plot, envir = globalenv())
        assign(paste0('hc_', method, '_k', i, '_silplot'), sil_plot, envir = globalenv())
        assign(paste0('hc_', method, '_k', i, '_dendrplot'), dendr_plot, envir = globalenv())
        
        rm(hc_res, hc_stats, sil, silinfo, df1, df2)
    }
}
```

### Dendrogram Plots
```{r}
par(mfrow = c(1,2)) 
hc_ward.D2_k2_dendrplot; hc_ward.D2_k3_dendrplot
hc_complete_k2_dendrplot; hc_complete_k3_dendrplot
hc_average_k2_dendrplot; hc_average_k3_dendrplot
```

### Cluster Plots
```{r}
par(mfrow = c(1,2)) 
hc_ward.D2_k2_clusplot; hc_ward.D2_k3_clusplot
hc_complete_k2_clusplot; hc_complete_k3_clusplot
hc_average_k2_clusplot; hc_average_k3_clusplot
```

### Summary Statistics
```{r}
myfun_getmeans(hc_ward.D2_k2$cluster, "Hierarchical Complete Linkage 2 Cluster Solution Developed PCA2 Data: Raw Score Means by Cluster")
myfun_getmeans(hc_complete_k2$cluster, "Hierarchical Complete Linkage 2 Cluster Solution Developed PCA2 Data: Raw Score Means by Cluster")
myfun_getmeans(hc_average_k2$cluster, "Hierarchical Average Linkage 2 Cluster Solution Developed PCA2 Data: Raw Score Means by Cluster")

myfun_getmeans(hc_ward.D2_k3$cluster, "Hierarchical Complete Linkage 3 Cluster Solution Developed PCA2 Data: Raw Score Means by Cluster")
myfun_getmeans(hc_complete_k3$cluster, "Hierarchical Complete Linkage 3 Cluster Solution Developed PCA2 Data: Raw Score Means by Cluster")
myfun_getmeans(hc_average_k3$cluster, "Hierarchical Average Linkage 3 Cluster Solution Developed PCA2 Data: Raw Score Means by Cluster")
```

### Write-up

There are three roughly equivalent solutions from the hierarchical clustering models. 2 cluster solutions appear to best fit the data. For the average linkage method, dimension reduction via PCA prior to clustering has minimal effect on the solution. For the complete linkage method, however, dimension reduction of the data prior to implementing the algorithm drastically alterst the result with the PCA transformation resulting in more even clusters comparable to the average linkage solution. Given that the average linkage method is consistent across data input formats (PCA or not), this solution is preferred.  


"Since there is no prediction method provided in the cluster package, cluster membership for nontraining instances has to be determined by calculating their dissimilarities to cluster medoids and choosing the least dissimilar medoid." Cichosz, P. (2015)

## Model Comparison
```{r}
clusstats_df <- do.call(rbind, mget(ls(pattern = "*clusstats")))
clusstats_df$`Average silhouette width` <- as.numeric(as.character(clusstats_df$`Average silhouette width`))
(table_cluster_descr <- kable(clusstats_df, 
      caption = "Cluster Descriptions for K-means", digits = 2, row.names = FALSE))

fit_df <- do.call(rbind, mget(ls(pattern = "*fit")))
varlist <- c("Average silhouette coefficient", "Average distance between clusters", "Average distance within clusters", "# observations with negative silhouette")

fit_df[, varlist] <- sapply(fit_df[varlist], function(x){
   as.numeric(as.character(x))
})
(table_cluster_fit <- kable(fit_df, 
      caption = "Evaluation of Cluster Goodness of Fit for K-means", digits = 2, row.names = FALSE))
```


```{r save objects}
save(data_matrix_aim1, km_k2_res, km_k3_res, pca_summary, pca_scaled, data_pca2components, variable_means_km2, variable_means_km3, file = "clustering_objects.Rda")
save(kmeans_kcca, file = "clustering_model_kmeans2.Rda")
save(table_cluster_descr, table_cluster_fit, fit_df, clusstats_df, file = "descriptives_tables.Rda")
```

