---
title: 'Hunter College Anxiety Dot-Probe: Machine Learning Regression Analysis'
author: "Krista DeStasio"
date: "11/06/2018"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: no
  pdf_document:
    latex_engine: xelatex
    toc: yes
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
# Clear workspace
rm(list = ls())

# Set paths and working directory
working_dir = '~/Dropbox/collaborations/hunter_college/hc_analysis/'
path_datafile = paste0(
    working_dir,
    '/data/N=844_FINAL_Traditional metrics_Trial Level metrics_questionnaires (n=837)_6.16.18.xlsx')
setwd(working_dir)

# Install and load required packages
list.of.packages <-
    c('janitor', 'caret', 'elasticnet', 'ranger', 'randomForest')
new.packages <-
    list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
if (length(new.packages))
    install.packages(new.packages, repos = "http://cran.us.r-project.org")
lapply(list.of.packages, library, character.only = TRUE)

# Knit options
knitr::opts_chunk$set(echo = TRUE)
```

# Data
The data are the first dot probe and the self-report questionnaires from the study pictured below.  

![Figure 1. Study Design](/Users/kristadestasio/Dropbox/collaborations/hunter_college/study_design.png)

## Dot Probe Measures 

- `rt_neutral_nt`: Average RT on all neutral trials  
- `rt_threat_nt`: Average RT on all threat trials  
- `rt_baseline`: Average RT on baseline trials where two neutral faces appear and there is no competition for attention between threat and neutral faces; These trials appear randomly throughout the task  
- `rt_outliers`: # of trials considered to be outliers based on the following criteria (RTs faster than 150 ms or slower than 2000 ms; any trial RT that was +/- 3 SD from the person’s mean RT for that trial type)  
- `mean_pos`: mean of positive trial-level threat bias scores  
- `mean_neg`: mean of negative trial-level threat bias scores      
- `peak_pos`: highest positive trial-level threat bias score
- `peak_neg`: highest negative trial-level threat bias score           
- `variability`: absolute value of the distance across all trial-level threat bias scores / number of pairs 

## Questionnaires

- **STAI**: The State-Trait Anxiety Inventory (STAI) is a measure consisted of 20 items assessing state and 20 items assessing trait anxiety.
* _Spielberger, C. D., Gorsuch, R. L., Lushene, R., Vagg, P. R., & Jacobs, G. A. (1983). Manual for the State-Trait Anxiety Inventory. Palo Alto, CA: Consulting Psychologists Press._  
- **BDI-II**: The Beck Depression Inventory-Second Edition (BDI-II) is a 21-item self-report inventory assessing the severity of depression over the past two week.  
* _Beck, A. T., Steer, R. A., & Brown, G. K. (1996). Beck Depression Inventory–II. San Antonio: Psychological Corporation._  
- **BAI**: The Beck Anxiety Inventory (BAI) is a 21-item self-report questionnaire measuring the general level of anxiety.  
* _Beck, A. T., Epstein, N., Brown, G., & Steer, R. A. (1988). An inventory for measuring clinical anxiety: Psychometric properties. Journal of Consulting and Clinical Psychology, 56(6), 893._  
- **MASQ**: The Mood and Anxiety Symptom Questionnaire (short version) is a 62-item questionnaire that measures anxious symptoms, anxious arousal, depressive symptoms, and anhedonic depression in the past week period.  
* _Watson, D., & Clark, L. A. (1991). The Mood and Anxiety Symptom Questionnaire. Unpublished manuscript, University of Iowa, Department of Psychology, Iowa City._  
- **PANAS**: The Positive and Negative Affect Schedule (PANAS) is a set of two 10-item scales that measure the current and general positive and negative affect.  
* _Watson, D., Clark, L. A., & Tellegen, A. (1988). Development and validation of brief measures of positive and negative affect: the PANAS scales. Journal of Personality and Social Psychology, 54(6), 1063._  

# Regression Analysis: Outcome measure of interest is stai_trait_totalscore  

**Analysis Plan: Supervised learning, regression analysis**

Since values of the outcome measure (total trait anxiety score on the State-Trait Anxiety Inventory) are known, we will use supervised learning. With a continuous outcome variable a regression model is appropriate.  

The data are to be split into a [training set, test set, and holdout set](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets). Will do an initial 80/20 split (development/holdout), then a subsequent 70/30 train/test split of the initial (80%) development set.  

The goal of this analysis is to identify those features that are useful for predicting anxiety scores (trait) as measured by the State-Trait Anxiety Inventory (STAI).  

In addition to testing that a model can be created to predict anxiety scores, we will conduct training on data frames of (1) all the predictors, (2) a subset without the dot probe measures, and (3) a subset without self-report measures. This will help us determine whether inclusion of the dot-probe and/or self-report questionnaires provides significant added predictive power. The best fitting model for each of these 3 data sets will be applied to the holdout sample. We will examine the final model to identify which predictors are most informative.  

## Steps, outline

1. Split the data into train/test sets and hold-out sets. Will do an initial 80/20 split (development/holdout), then a subsequent 70/30 train/test split of the initial (80%) development set.  
2. Data cleaning/pre-processing
3. Initial model specification  
- Select model parameters. (i.e., coefficients in linear regression)  
- Baseline STAI anxiety score    
- Baseline dot probe  
- Select algorithms  
- linear regression  
- Lasso  
- Elastic net  
- Random forest  
- Cross-validation  
- k-fold cross validation with 10 folds  
4. Model testing on training set  
5. Model testing on test set
6. Adjust model parameters as needed - set hyperparameters. (repeat steps 3-5 until satisfied).  
7. Test final model on the hold-out sample.  
8. Evaluate model performance.  

## Data Cleaning
```{r Read in the data, include=FALSE}
dataframe_session1 <-
    clean_names(readxl::read_excel(path_datafile, sheet = 1, col_names = TRUE))
dataframe_questionnaires <-
    clean_names(readxl::read_excel(
        path_datafile,
        sheet = 2,
        col_names = TRUE,
        skip = 1))
dataframe_tlmetrics <-
    clean_names(readxl::read_excel(path_datafile, sheet = 1, col_names = TRUE))
```
```{r Check the data, include=FALSE}
str(dataframe_session1)
str(dataframe_questionnaires)
str(dataframe_tlmetrics)

table(c(
    colnames(dataframe_session1),
    colnames(dataframe_questionnaires),
    colnames(dataframe_tlmetrics)))
```

### Three variables are dropped from this data set

1. `session number` - all data are from a single session, making this variable irrelevant.  
2. `date of birth` - age is also a variable in this data set. DOB is ommitted as it would be perfectly correlated with age, which is retained.  
3. `subject id` - this is an identification tag, not a meaningful variable.  

```{r Create full data frame, include=FALSE}
abmt_data <- clean_names(readxl::read_excel(path_datafile))
# Drop the session and date of birth columns
abmt_data <-
    subset(abmt_data, select = -c(session, dob, subject_id))
# Display number of observations & features
dim(abmt_data)
```

### Missing data  
Machine learning can be used only on data sets without missing values. Rows with missing values are therefore omitted.  
```{r Drop missing values, include=FALSE}
abmt_data[abmt_data == 9999] <- NA
abmt_data[abmt_data == '#NULL!'] <- NA
missing_values = abmt_data[!complete.cases(abmt_data),]
missing_values[, 1] # These participants are missing data and are ommitted from the analyses
abmt_data = na.omit(abmt_data)
```
```{r Assign the variable types, include=FALSE}
abmt_data$gender <- as.factor(abmt_data$gender)
# Create dummy variables
abmt_data$gender <-
    model.matrix(stai_trait_totalscore ~ ., data = abmt_data[, 1:length(abmt_data)])[, 17]
```

## Data split 1
### 80/20 split (development/holdout)  
The data are split into two initial subsets. 80% of the full data are used for model training and testing 20% of the data form the holdout sample. The holdout sample is set aside until we decide on a final model. Once training and testing are complete and a final model selected, model performance is assesed by applying it to the holdout test set.  

```{r Split the data into holdout based on outcome, echo=FALSE}
set.seed(50)  # set.seed is a random number generator; the value in parentheses is arbitrary, and a seed is only set so that we can reproduce these same results next time we run the analysis.

# Split the original dataset into a training set and a testing set
partition_data <-
    createDataPartition(
        abmt_data$stai_trait_totalscore,
        times = 1,
        p = .8,
        list = FALSE)

development_set <- abmt_data[partition_data, ] # development set
holdout_set <- abmt_data[-partition_data, ] # holdout set
```

## Pre-Processing (development set)
### Zero and near zero variance
None of the variables exhibit zero or near-zero variance.  
```{r Calculate variance metrics, include=FALSE}
# http://topepo.github.io/caret/pre-processing.html
# Frequency ratio & % of unique values
(nzv = nearZeroVar(development_set, saveMetrics = TRUE))
```

### Linear dependencies
```{r Linear dependency, include=FALSE}
# Identify linear combinations
(comboInfo = findLinearCombos(development_set))
colnames(development_set[comboInfo$remove])

# Remove linear dependencies
development_set <- development_set[, -comboInfo$remove]
```
The following are linear combinations of other variables and are removed from the data set: `r colnames(development_set[comboInfo$remove])`  

## Data split 2
### 70/30 train/test split of the initial (80%) development set  
Three training data sets are created for the development of 3 different models. They are split as follows:    

```{r Split the data based on the outcome, results="hide"}
# See http://topepo.github.io/caret/data-splitting.html for more on splitting data
set.seed(50)
partition_data <-
    createDataPartition(
        development_set$stai_trait_totalscore,
        times = 1,
        p = .7,
        list = FALSE)

training_set <- as.data.frame(development_set[partition_data,])
test_set <- as.data.frame(development_set[-partition_data,])

# Without dot prob variables
training_set_nodot <-
    as.data.frame(development_set[partition_data, 13:length(development_set)])

# Without self-report questionnaire variables
training_set_noself <-
    as.data.frame(development_set[partition_data, c(1:14, 16)])
```

# Scale the training and testing data sets
```{r}
ind <- sapply(training_set_nodot, is.numeric)
training_set_nodot[ind] <- lapply(training_set_nodot[ind], scale)
training_set_nodot[ind] <- lapply(training_set_nodot[ind], as.numeric)

ind <- sapply(training_set_noself, is.numeric)
training_set_noself[ind] <- lapply(training_set_noself[ind], scale)
training_set_noself[ind] <- lapply(training_set_noself[ind], as.numeric)

ind <- sapply(test_set, is.numeric)
test_set[ind] <- lapply(test_set[ind], scale)
test_set[ind] <- lapply(test_set[ind], as.numeric)
```

## Data visualization 
### Rank of matrices

["A matrix is said to have full rank if its rank is either equal to its number of columns or to its number of rows (or to both)."](https://stackoverflow.com/questions/26558631/predict-lm-in-a-loop-warning-prediction-from-a-rank-deficient-fit-may-be-mis)  

```{r Matrix rank, include=FALSE}
# Check the rank of the data matrices
library(Matrix)
cat(rankMatrix(training_set), "\n")
cat(rankMatrix(test_set), "\n")
cat(rankMatrix(training_set_nodot), "\n")
cat(rankMatrix(training_set_noself), "\n")
```

**Full feature data set**: Both the training and test sets achieve full rank with a rank of `r rankMatrix(training_set)[1]`.  

**Data set without dot probe**: The training set achieves full rank with a rank of `r rankMatrix(training_set_nodot)[1]`.  

**Data set without self-report measures**: The training set achieves full rank with a rank of`r rankMatrix(training_set_noself)[1]`.  

### Data set description

**# of Observations, # of Features**

**Training set all predictors:** `r dim(training_set)`; train the model & tune parameters on this sample  
**Training set no dot probe:** `r dim(training_set_nodot)`; train the model & tune parameters on this sample   
**Training set no self-report questionnaires:** `r dim(training_set_noself)`; train the model & tune parameters on this sample  
**Testing set:** `r dim(test_set)`; initial model test and adjustment on this sample  
**Holdout set:** `r dim(holdout_set)`; hold-out sample, final test of model  

#### Data structure
```{r Look at the data, echo=FALSE}
str(development_set)
```

### Feature plots of each predictor with the outcome measure (stai_trait)
```{r Data visualization, echo=FALSE}
# Assign the regression features to an object
regVar <- colnames(development_set[,c(1:ncol(development_set))])

theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.4, .4, .4, .8)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)

featurePlot(x = development_set[, regVar][1:3], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][4:6], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][7:9], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][10:12], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][13:15], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][16:18], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][19:21], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][22:24], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][25:26], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
```

## Model training

- This website suggest regression with random forest: https://hackernoon.com/choosing-the-right-machine-learning-algorithm-68126944ce1f  

- Scikit learn recommends regression with [Lasso](https://pdfs.semanticscholar.org/6b5e/99c128b9cd7b7fbc817a2843a47ce8a1c35d.pdf) or [Elastic Net](https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf)

### Cross test
k-fold cross test is used with 10 folds.  
```{r k-fold resampling, include=FALSE}
# Specify the type of resampling
fit_kfold <-
    trainControl(method = 'cv',
                 number = 10,
                 returnResamp = "final")
```

### Linear regression
The absolute value of the t-statistic for each model parameter is used to estimate the contribution of each variable to the model.  
```{r Linear regression basic model}
set.seed(50)
training_dfs_list <-
    list(training_set, training_set_nodot, training_set_noself)

lmFit_kfold <- lapply(training_dfs_list, function(x)
    train(
        stai_trait_totalscore ~ . ,
        data = x,
        method = "lm",
        trControl = fit_kfold))
```

#### Summaries
##### All features
```{r summary lm all features, echo=FALSE}
summary(lmFit_kfold[1])
```

##### No dot probe
```{r summary lm no dot probe, echo=FALSE}
summary(lmFit_kfold[2])
```

##### No self-report questionnaires
```{r summary lm no SR, echo=FALSE}
summary(lmFit_kfold[3])
```

### Regularization regression methods
[L1 and L2 regularization methods](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) are used in Lasso, Ridge, and Elastic Net Regression. See a discussion of Ridge, Lasso, and Elastic Net Regularization [here.](http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/)  

### Lasso
Regularization can help prevent overfitting, which in our case is important due to the low n. ["The amount of the penalty can be fine-tuned using a constant called lambda (λ). Selecting a good value for λ is critical."](http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/)  

```{r Lasso}
# tuning parameters: Fraction of Full Solution (fraction, numeric)
set.seed(50)
lassoFit_kfold <- lapply(training_dfs_list, function(x)
    train(
        stai_trait_totalscore ~ . ,
        data = x,
        method = "lasso",
        trControl = fit_kfold)) 
```

#### Summaries
##### All features
```{r lasso summary all, echo=FALSE}
lassoFit_kfold[[1]]
```

#### No dot probe
```{r lasso summary no dot, echo=FALSE}
lassoFit_kfold[[2]]
```

#### No self-report questionnaires
```{r lasso summary no SR, echo=FALSE}
lassoFit_kfold[[3]]
```

#### Plots
##### All features
```{r plot lasso all features, echo=FALSE}
trellis.par.set(caretTheme())
ggplot(lassoFit_kfold[[1]])
```

##### No dot probe
```{r plot lass not dot, echo=FALSE}
ggplot(lassoFit_kfold[[2]])
```

##### No self-report questionnaires
```{r plot lass not SR, echo=FALSE}
ggplot(lassoFit_kfold[[3]])
```

### Elastic net
["Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO)."](http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/) Because Lasso and Elastic Net shrink coefficient estimates with little contribution to zero, it can serve as an alternative to feature selection methods.  

```{r elastic net}
# Tuning parameters: Fraction of Full Solution (fraction, numeric) and Weight Decay (lambda, numeric)
set.seed(50)

enetFit_kfold <- lapply(training_dfs_list, function(x)
    train(
        stai_trait_totalscore ~ . ,
        data = x,
        method = "enet",
        trControl = fit_kfold))
```

#### Summaries
##### All features
```{r enet summary all, echo=FALSE}
enetFit_kfold[[1]]
```

##### No dot probe
```{r enet summary nodot, echo=FALSE}
enetFit_kfold[[2]]
```

##### No self-report questionnaires
```{r enet summary SR, echo=FALSE}
enetFit_kfold[[3]]
```

#### Plots
##### All features
```{r plot enet, echo=FALSE}
trellis.par.set(caretTheme())
ggplot(enetFit_kfold[[1]])
```

##### No dot probe
```{r plot enet no dot, echo=FALSE}
ggplot(enetFit_kfold[[2]])
```

##### No self-report questionnaires
```{r plot enet no SR, echo=FALSE}
ggplot(enetFit_kfold[[3]])
```

### Random Forest
["Random Forest: from the R package: 'For each tree, the prediction accuracy on the out-of-bag portion of the data is recorded. Then the same is done after permuting each predictor variable. The difference between the two accuracies are then averaged over all trees, and normalized by the standard error. For regression, the MSE is computed on the out-of-bag data for each tree, and then the same computed after permuting a variable. The differences are averaged and normalized by the standard error. If the standard error is equal to 0 for a variable, the division is not done.'”](http://topepo.github.io/caret/variable-importance.html)

```{r Random forest}
set.seed(50)
rfFit_kfold <- lapply(training_dfs_list, function(x)
    train(
        stai_trait_totalscore ~ .,
        data = x,
        method = 'rf',
        trControl = fit_kfold))
```

#### Summaries
##### All features
```{r rf summary all, echo=FALSE}
rfFit_kfold[[1]]
```

#### No dot probe
```{r rf summary nodot, echo=FALSE}
rfFit_kfold[[2]]
```

##### No self-report questionnaires
```{r rf summary no SR, echo=FALSE}
rfFit_kfold[[3]]
```

#### Plots
##### All features
```{r plot rf, echo=FALSE}
trellis.par.set(caretTheme())
ggplot(rfFit_kfold[[1]])
```

##### No dot probe
```{r plot rf no dot probe, echo=FALSE}
ggplot(rfFit_kfold[[2]])
```

##### No self-report questionnaires
```{r plot rf no SR, echo=FALSE}
ggplot(rfFit_kfold[[3]])
```

## Comparison of model performance on training set
### Summaries
```{r Model summaries comparison training set, echo=FALSE}
# Resampling results
resamps <- resamples(
    list(
        LM_all = lmFit_kfold[[1]],
        LASSO_all = lassoFit_kfold[[1]],
        ENET_all = enetFit_kfold[[1]],
        RF_all = rfFit_kfold[[1]],
        LM_nodot = lmFit_kfold[[2]],
        LASSO_nodot = lassoFit_kfold[[2]],
        ENET_nodot = enetFit_kfold[[2]],
        RF_nodot = rfFit_kfold[[2]],
        LM_noself = lmFit_kfold[[3]],
        LASSO_noself = lassoFit_kfold[[3]],
        ENET_noself = enetFit_kfold[[3]],
        RF_noself = rfFit_kfold[[3]]))
summary(resamps)
```

### Plots
```{r plot model comparisons, echo=FALSE}
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))
```

### Difference table and plots
```{r Model diff, echo=FALSE}
difValues <- diff(resamps)
summary(difValues)

theme2 <- trellis.par.get()
theme2$plot.symbol$col = rgb(.4, .4, .4, .8)
theme2$plot.symbol$pch = 16
theme2$plot.line$col = rgb(1, 0, 0, .7)
theme2$plot.line$lwd <- 2
theme2$fontsize$text <- 5
trellis.par.set(theme2)
bwplot(difValues, layout = c(3, 1))
```

## Model testing on test set
<!-- ##### Linear regression -->
```{r lm predict values in test from training model, include=FALSE}
lm_predict <- predict(lmFit_kfold[[1]], test_set) 
postResample(lm_predict, test_set$stai_trait_totalscore)
```
```{r lm predict values in test from training model no dot, include=FALSE}
lm_predict_nodot <- predict(lmFit_kfold[[2]], test_set) 
postResample(lm_predict_nodot, test_set$stai_trait_totalscore)
```
```{r lm predict values in test from training model no SR, include=FALSE}
lm_predict_noself <- predict(lmFit_kfold[[3]], test_set) 
postResample(lm_predict_noself, test_set$stai_trait_totalscore)
```

<!-- ##### Lasso -->
```{r lasso predict values in test from training model, include=FALSE}
set.seed(50)
lasso_predict <- predict(lassoFit_kfold[[1]], test_set)
postResample(lasso_predict, test_set$stai_trait_totalscore)
```
```{r lasso predict values in test from training model no dot, include=FALSE}
lasso_predict_nodot <- predict(lassoFit_kfold[[2]], test_set)
postResample(lasso_predict_nodot, test_set$stai_trait_totalscore)
```
```{r lasso predict values in test from training model no SR, include=FALSE}
lasso_predict_noself <- predict(lassoFit_kfold[[3]], test_set)
postResample(lasso_predict_noself, test_set$stai_trait_totalscore)
```

<!-- ##### Elastic net -->
```{r enet predict all, include=FALSE}
enet_predict <- predict(enetFit_kfold[[1]], test_set)
postResample(enet_predict, test_set$stai_trait_totalscore)
```
```{r enet predict nodot, include=FALSE}
enet_predict_nodot <- predict(enetFit_kfold[[2]], test_set)
postResample(enet_predict_nodot, test_set$stai_trait_totalscore)
```
```{r enet predict noself, include=FALSE}
enet_predict_noself <- predict(enetFit_kfold[[3]], test_set)
postResample(enet_predict_noself, test_set$stai_trait_totalscore)
```

<!-- ##### Random forest -->
```{r rf predict values in test from training model, include=FALSE}
rf_predict <- predict(rfFit_kfold[[1]], test_set)
postResample(rf_predict, test_set$stai_trait_totalscore)
```
```{r rf predict values in test from training model no dot, include=FALSE}
rf_predict_nodot <- predict(rfFit_kfold[[2]], test_set)
postResample(rf_predict_nodot, test_set$stai_trait_totalscore)
```
```{r rf predict values in test from training model no SR, include=FALSE}
rf_predict_noself <- predict(rfFit_kfold[[3]], test_set)
postResample(rf_predict_noself, test_set$stai_trait_totalscore)
```

### Comparison table of model performance on the training and test sets with all features
```{r Model performance test set, echo=FALSE, warning=FALSE}
RMSE_Training <- c(round(lmFit_kfold[[1]]$results[1,2], 2), 
                   round(lassoFit_kfold[[1]]$results[1,2], 2), 
                   round(enetFit_kfold[[1]]$results[1,3], 2), 
                   round(rfFit_kfold[[1]]$results[1,2], 2))

Rsq_Training <-  c(round(lmFit_kfold[[1]]$results[1,3], 2), 
                   round(lassoFit_kfold[[1]]$results[1,3], 2), 
                   round(enetFit_kfold[[1]]$results[1,4], 2), 
                   round(rfFit_kfold[[1]]$results[1,3],2))

RMSE_Test <- c(round(postResample(lm_predict, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(lasso_predict, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(enet_predict, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(rf_predict, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test <- c(round(postResample(lm_predict, test_set$stai_trait_totalscore)[2], 2), 
              round(postResample(lasso_predict, test_set$stai_trait_totalscore )[2], 2), 
              round(postResample(enet_predict, test_set$stai_trait_totalscore)[2], 2), 
              round(postResample(rf_predict, test_set$stai_trait_totalscore)[2], 2))

model_names <- c("Linear Regression", "Lasso", "Elastic Net", "Random Forest")

difference_rmse <- as.numeric(RMSE_Test) - as.numeric(RMSE_Training)
difference_rsq <- as.numeric(Rsq_Test) - as.numeric(Rsq_Training)

knitr::kable(data.frame(
    cbind(
        model_names,
        RMSE_Training,
        Rsq_Training,
        RMSE_Test,
        Rsq_Test,
        difference_rmse,
        difference_rsq)), 
    caption = 'Summary stats model performance all features', row.names = FALSE)
```

### Comparison table of model performance on the training and test sets without dot probe
```{r Model performance test set no dot, echo=FALSE, warning=FALSE}
RMSE_Training <- c(round(lmFit_kfold[[2]]$results[1,2], 2),
                   round(lassoFit_kfold[[2]]$results[1,2], 2),
                   round(enetFit_kfold[[2]]$results[1,3], 2),
                   round(rfFit_kfold[[2]]$results[1,2], 2))

Rsq_Training <-  c(round(lmFit_kfold[[2]]$results[1,3], 2), 
                   round(lassoFit_kfold[[2]]$results[1,3], 2), 
                   round(enetFit_kfold[[2]]$results[1,4], 2), 
                   round(rfFit_kfold[[2]]$results[1,3],2))

RMSE_Test <- c(round(postResample(lm_predict_nodot, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(lasso_predict_nodot, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(enet_predict_nodot, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(rf_predict_nodot, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test <- c(round(postResample(lm_predict_nodot, test_set$stai_trait_totalscore)[2], 2),
              round(postResample(lasso_predict_nodot, test_set$stai_trait_totalscore )[2], 2),
              round(postResample(enet_predict_nodot, test_set$stai_trait_totalscore)[2], 2),
              round(postResample(rf_predict_nodot, test_set$stai_trait_totalscore)[2], 2))
model_names <- c("Linear Regression", "Lasso", "Elastic Net", "Random Forest")

difference_rmse <- as.numeric(RMSE_Test) - as.numeric(RMSE_Training)
difference_rsq <- as.numeric(Rsq_Test) - as.numeric(Rsq_Training)

knitr::kable(data.frame(
    cbind(
        model_names,
        RMSE_Training,
        Rsq_Training,
        RMSE_Test,
        Rsq_Test,
        difference_rmse,
        difference_rsq)), 
    caption = 'Summary stats model performance no dot probe', row.names = FALSE)
```

### Comparison table of model performance on the training and test sets without self-report questionnaires
```{r Model performance test set noself, echo=FALSE, warning=FALSE}
RMSE_Training <- c(round(lmFit_kfold[[3]]$results[1,2], 2), 
                   round(lassoFit_kfold[[3]]$results[1,2], 2), 
                   round(enetFit_kfold[[3]]$results[1,3], 2), 
                   round(rfFit_kfold[[3]]$results[1,2], 2))

Rsq_Training <-  c(round(lmFit_kfold[[3]]$results[1,3], 2), round(lassoFit_kfold[[3]]$results[1,3], 2), round(enetFit_kfold[[3]]$results[1,4], 2), round(rfFit_kfold[[3]]$results[1,3],2))

RMSE_Test <- c(round(postResample(lm_predict_noself, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(lasso_predict_noself, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(enet_predict_noself, test_set$stai_trait_totalscore)[1], 2),
               round(postResample(rf_predict_noself, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test <- c(round(postResample(lm_predict_noself, test_set$stai_trait_totalscore)[2], 2), 
              round(postResample(lasso_predict_noself, test_set$stai_trait_totalscore )[2], 2), 
              round(postResample(enet_predict_noself, test_set$stai_trait_totalscore)[2], 2), 
              round(postResample(rf_predict_noself, test_set$stai_trait_totalscore)[2], 2))
model_names <- c("Linear Regression", "Lasso", "Elastic Net", "Random Forest")

difference_rmse <- as.numeric(RMSE_Test) - as.numeric(RMSE_Training)
difference_rsq <- as.numeric(Rsq_Test) - as.numeric(Rsq_Training)

knitr::kable(data.frame(
    cbind(
        model_names,
        RMSE_Training,
        Rsq_Training,
        RMSE_Test,
        Rsq_Test,
        difference_rmse,
        difference_rsq)), 
    caption = 'Summary stats model performance no self-report questionnaires', row.names = FALSE)
```

## Parameter tuning, all features
### Lasso model tuning, all features
```{r Tune Lasso}
#lasso_info <- getModelInfo("lasso")
#lasso_info$lasso$parameters
tune_grid <- expand.grid(fraction = c(0.5, 0.6, 0.625, 0.65, 0.7, 0.725, 0.7, 0.8, 0.9))

set.seed(50)
lassoFit_tune <-
    train(
        stai_trait_totalscore ~ .,
        data = training_set,
        method = 'lasso',
        trControl = fit_kfold,
        tuneGrid = tune_grid)
plot(lassoFit_tune)  
```

RMSE was used to select the optimal model using the smallest value. The final values used for the model were fraction = `r lassoFit_tune$bestTune$fraction`.  

### Elastic Net model tuning, all features
```{r Tune enet}
set.seed(50)
enetFit_tune <-
    train(
        stai_trait_totalscore ~ .,
        data = training_set,
        method = 'enet',
        trControl = fit_kfold,
        tuneLength = 20)
trellis.par.set(caretTheme())
plot(enetFit_tune)  
```

RMSE was used to select the optimal model using the smallest value. The final values used for the model were fraction = `r enetFit_tune$bestTune$fraction` and lambda = `r enetFit_tune$bestTune$lambda`.  

### Random Forest model tuning, all features
```{r Tune RF}
#rf_info <- getModelInfo("rf")
#rf_info$rf$parameters
tune_grid <- expand.grid(mtry = c(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16))

set.seed(50)
(rfFit_tune <- train(
    stai_trait_totalscore ~ .,
    data = training_set,
    method = 'rf',
    trControl = fit_kfold,
    tuneGrid = tune_grid))
```

## All features: Post-tuning model testing on test set
```{r lasso all, include=FALSE}
lasso_predict2 <- predict(lassoFit_tune, test_set)
postResample(lasso_predict2, test_set$stai_trait_totalscore)
```
```{r enet all, include=FALSE}
enet_predict2 <- predict(enetFit_tune, test_set)
postResample(enet_predict2, test_set$stai_trait_totalscore)
```
```{r rf all, include=FALSE}
rf_predict2 <- predict(rfFit_tune, test_set)
postResample(rf_predict2, test_set$stai_trait_totalscore)
```

```{r Second model comparison, echo=FALSE, warning=FALSE}
RMSE_Training <- c(round(lmFit_kfold[[1]]$results[1,2], 2),
                   round(lassoFit_kfold[[1]]$results[1,2], 2),
                   round(enetFit_kfold[[1]]$results[1,3], 2),
                   round(rfFit_kfold[[1]]$results[1,2], 2))

Rsq_Training <-  c(round(lmFit_kfold[[1]]$results[1,3], 2),
                   round(lassoFit_kfold[[1]]$results[1,3], 2),
                   round(enetFit_kfold[[1]]$results[1,4], 2),
                   round(rfFit_kfold[[1]]$results[1,3], 2))

RMSE_Test1 <- c(round(postResample(lm_predict, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(lasso_predict, test_set$stai_trait_totalscore )[1], 2),
                round(postResample(enet_predict, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(rf_predict, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test1 <- c(round(postResample(lm_predict, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(lasso_predict, test_set$stai_trait_totalscore )[2], 2),
               round(postResample(enet_predict, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(rf_predict, test_set$stai_trait_totalscore)[2], 2))

RMSE_Test2 <- c(NA, round(postResample(lasso_predict2, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(enet_predict2, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(rf_predict2, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test2 <- c(NA, round(postResample(lasso_predict2, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(enet_predict2, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(rf_predict2, test_set$stai_trait_totalscore)[2], 2))
model_names <- c("Linear Regression", "Lasso", "Elastic Net", "Random Forest")

difference_rmse_TV1 <- round((as.numeric(RMSE_Test) - as.numeric(RMSE_Training)), 2)
difference_rsq_TV1 <- round((as.numeric(Rsq_Test) - as.numeric(Rsq_Training)), 2)

difference_rmse_TV2 <- round((as.numeric(RMSE_Test2) - as.numeric(RMSE_Training)), 2)
difference_rsq_TV2 <- round((as.numeric(Rsq_Test2) - as.numeric(Rsq_Training)), 2)

difference_rmse_V1V2 <- round((as.numeric(RMSE_Test2) - as.numeric(RMSE_Test1)), 2)
difference_rsq_V1V2 <- round((as.numeric(Rsq_Test2) - as.numeric(Rsq_Test1)), 2)

knitr::kable(data.frame(
    cbind(
        model_names,
        RMSE_Training,
        RMSE_Test1,
        difference_rmse_TV1,
        RMSE_Test2,
        difference_rmse_TV2,
        difference_rmse_V1V2,
        Rsq_Training,
        Rsq_Test1,
        difference_rsq_TV1,
        Rsq_Test2,
        difference_rsq_TV2,
        difference_rsq_V1V2)), 
    caption = 'Summary stats model performance all features', row.names = FALSE)
```

### Model difference estimates and p-values, all features
```{r Model difference comparison, echo=FALSE}
resamps <- resamples(
    list(
        LM = lmFit_kfold[[1]],
        LASSO = lassoFit_kfold[[1]],
        ENET = enetFit_kfold[[1]],
        RF = rfFit_kfold[[1]],
        LASSO_tune = lassoFit_tune,
        ENET_tune = enetFit_tune,
        RF_tune = rfFit_tune))
summary(diff(resamps))
```

```{r plot untuned and tuned model difference comparison}
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))
```

## No dot probe: Parameter tuning
### Lasso model tuning, no dot probe
```{r Tune lasso nodot}
tune_grid <- expand.grid(fraction = c(0.8, 0.825, 0.85, 0.875, 0.9, 0.925, 0.95, 0.975, 1))

set.seed(50)
lassoFit_tune_nodot <-
    train(
        stai_trait_totalscore ~ .,
        data = training_set_nodot,
        method = 'lasso',
        trControl = fit_kfold,
        tuneGrid = tune_grid)
plot(lassoFit_tune_nodot)  
```

RMSE was used to select the optimal model using the smallest value. The final values used for the model were fraction = `r lassoFit_tune_nodot$bestTune$fraction`.  

### Elastic Net model tuning, no dot probe
```{r Tune enet nodot}
set.seed(50)
enetFit_tune_nodot <-
    train(
        stai_trait_totalscore ~ .,
        data = training_set_nodot,
        method = 'enet',
        trControl = fit_kfold,
        tuneLength = 20)
trellis.par.set(caretTheme())
plot(enetFit_tune_nodot)  
```

RMSE was used to select the optimal model using the smallest value. The final values used for the model were fraction = `r enetFit_tune_nodot$bestTune$fraction` and lambda = `r enetFit_tune_nodot$bestTune$lambda`.  

### Random Forest model tuning, no dot probe
```{r Tune RF nodot}
tune_grid <- expand.grid(mtry = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))

set.seed(50)
(rfFit_tune_nodot <-
        train(
            stai_trait_totalscore ~ .,
            data = training_set,
            method = 'rf',
            trControl = fit_kfold,
            tuneGrid = tune_grid))
```

## No dot probe: Post-tuning model testing on test set
```{r lasso nodot, include=FALSE}
lasso_predict2_nodot <- predict(lassoFit_tune_nodot, test_set)
postResample(lasso_predict2_nodot, test_set$stai_trait_totalscore)
```
```{r enet nodot, include=FALSE}
enet_predict2_nodot <- predict(enetFit_tune_nodot, test_set)
postResample(enet_predict2_nodot, test_set$stai_trait_totalscore)
```
```{r rf nodot, include=FALSE}
rf_predict2_nodot <- predict(rfFit_tune_nodot, test_set)
postResample(rf_predict2_nodot, test_set$stai_trait_totalscore)
```
```{r Second model comparison nodot, echo=FALSE, warning=FALSE}
RMSE_Training <- c(round(lmFit_kfold[[2]]$results[1,2], 2),
                   round(lassoFit_kfold[[2]]$results[1,2], 2),
                   round(enetFit_kfold[[2]]$results[1,3], 2),
                   round(rfFit_kfold[[2]]$results[1,2], 2))

Rsq_Training <-  c(round(lmFit_kfold[[2]]$results[1,3], 2),
                   round(lassoFit_kfold[[2]]$results[1,3], 2),
                   round(enetFit_kfold[[2]]$results[1,4], 2),
                   round(rfFit_kfold[[2]]$results[1,3], 2))

RMSE_Test1 <- c(round(postResample(lm_predict_nodot, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(lasso_predict_nodot, test_set$stai_trait_totalscore )[1], 2),
                round(postResample(enet_predict_nodot, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(rf_predict_nodot, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test1 <- c(round(postResample(lm_predict_nodot, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(lasso_predict_nodot, test_set$stai_trait_totalscore )[2], 2),
               round(postResample(enet_predict_nodot, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(rf_predict_nodot, test_set$stai_trait_totalscore)[2], 2))

RMSE_Test2 <- c(NA, round(postResample(lasso_predict2_nodot, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(enet_predict2_nodot, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(rf_predict2_nodot, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test2 <- c(NA, round(postResample(lasso_predict2_nodot, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(enet_predict2_nodot, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(rf_predict2_nodot, test_set$stai_trait_totalscore)[2], 2))
model_names <- c("Linear Regression", "Lasso", "Elastic Net", "Random Forest")

difference_rmse_TV1 <- round((as.numeric(RMSE_Test) - as.numeric(RMSE_Training)), 2)
difference_rsq_TV1 <- round((as.numeric(Rsq_Test) - as.numeric(Rsq_Training)), 2)

difference_rmse_TV2 <- round((as.numeric(RMSE_Test2) - as.numeric(RMSE_Training)), 2)
difference_rsq_TV2 <- round((as.numeric(Rsq_Test2) - as.numeric(Rsq_Training)), 2)

difference_rmse_V1V2 <- round((as.numeric(RMSE_Test2) - as.numeric(RMSE_Test1)), 2)
difference_rsq_V1V2 <- round((as.numeric(Rsq_Test2) - as.numeric(Rsq_Test1)), 2)

knitr::kable(data.frame(
    cbind(
        model_names,
        RMSE_Training,
        RMSE_Test1,
        difference_rmse_TV1,
        RMSE_Test2,
        difference_rmse_TV2,
        difference_rmse_V1V2,
        Rsq_Training,
        Rsq_Test1,
        difference_rsq_TV1,
        Rsq_Test2,
        difference_rsq_TV2,
        difference_rsq_V1V2)), 
    caption = 'Comparison of model and difference scores: No dot probe', row.names = FALSE)
```

### Model difference estimates and p-values, no dot probe
```{r Model difference comparison nodot, echo=FALSE}
resamps <- resamples(
    list(
        LM = lmFit_kfold[[2]],
        LASSO = lassoFit_kfold[[2]],
        ENET = enetFit_kfold[[2]],
        RF = rfFit_kfold[[2]],
        LASSO_tune_nodot = lassoFit_tune_nodot,
        ENET_tune_nodot = enetFit_tune_nodot,
        RF_tune_nodot = rfFit_tune_nodot))
summary(diff(resamps))
```

```{r plot untuned and tuned model difference comparison nodot, echo=FALSE}
theme1 <- trellis.par.get()
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))
```

## No self-report: Parameter tuning
### Lasso model tuning, no self-report questionnaires
```{r Tune lasso noself}
tune_grid <- expand.grid(fraction = c(0, 0.25, 0.5, 0.75, 0.1, 0.125, 0.15, 0.175, .2, .25, .3, .35, .4, .45, .5))

set.seed(50)
lassoFit_tune_noself <-
    train(
        stai_trait_totalscore ~ .,
        data = training_set_noself,
        method = 'lasso',
        trControl = fit_kfold,
        tuneGrid = tune_grid)
plot(lassoFit_tune_noself)  
```

RMSE was used to select the optimal model using the smallest value. The final values used for the model were fraction = `r lassoFit_tune_noself$bestTune$fraction`.  

### Elastic Net model tuning, no self-report questionnaires
```{r Tune enet noself}
set.seed(50)
enetFit_tune_noself <-
    train(
        stai_trait_totalscore ~ .,
        data = training_set_noself,
        method = 'enet',
        trControl = fit_kfold,
        tuneLength = 20)
trellis.par.set(caretTheme())
plot(enetFit_tune_noself)  
```

RMSE was used to select the optimal model using the smallest value. The final values used for the model were fraction = `r enetFit_tune_noself$bestTune$fraction` and lambda = `r enetFit_tune_noself$bestTune$lambda`.  

### Random Forest model tuning, no self-report questionnaires
```{r Tune RF noself}
tune_grid <- expand.grid(mtry = c(2, 3, 4, 5, 6, 7, 8))

set.seed(50)
(rfFit_tune_noself <-
        train(
            stai_trait_totalscore ~ .,
            data = training_set_noself,
            method = 'rf',
            trControl = fit_kfold,
            tuneGrid = tune_grid,
            importance = TRUE))
```

## Post-tuning model testing on test set, no self-report questionnaires
```{r lasso noself, include=FALSE}
lasso_predict2_noself <- predict(lassoFit_tune_noself, test_set)
postResample(lasso_predict2_noself, test_set$stai_trait_totalscore)
```
```{r enet noself, include=FALSE}
enet_predict2_noself <- predict(enetFit_tune_noself, test_set)
postResample(enet_predict2_noself, test_set$stai_trait_totalscore)
```
```{r RF noself, include=FALSE}
rf_predict2_noself <- predict(rfFit_tune_noself, test_set)
postResample(rf_predict2_noself, test_set$stai_trait_totalscore)
```

```{r Second model comparison noself, echo=FALSE, warning=FALSE}
RMSE_Training <- c(round(lmFit_kfold[[3]]$results[1,2], 2),
                   round(lassoFit_kfold[[3]]$results[1,2], 2),
                   round(enetFit_kfold[[3]]$results[1,3], 2),
                   round(rfFit_kfold[[3]]$results[1,2], 2))

Rsq_Training <-  c(round(lmFit_kfold[[3]]$results[1,3], 2),
                   round(lassoFit_kfold[[3]]$results[1,3], 2),
                   round(enetFit_kfold[[3]]$results[1,4], 2),
                   round(rfFit_kfold[[3]]$results[1,3], 2))

RMSE_Test1 <- c(round(postResample(lm_predict_noself, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(lasso_predict_noself, test_set$stai_trait_totalscore )[1], 2),
                round(postResample(enet_predict_noself, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(rf_predict_noself, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test1 <- c(round(postResample(lm_predict_noself, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(lasso_predict_noself, test_set$stai_trait_totalscore )[2], 2),
               round(postResample(enet_predict_noself, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(rf_predict_noself, test_set$stai_trait_totalscore)[2], 2))

RMSE_Test2 <- c(NA, round(postResample(lasso_predict2_noself, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(enet_predict2_noself, test_set$stai_trait_totalscore)[1], 2),
                round(postResample(rf_predict2_noself, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test2 <- c(NA, round(postResample(lasso_predict2_noself, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(enet_predict2_noself, test_set$stai_trait_totalscore)[2], 2),
               round(postResample(rf_predict2_noself, test_set$stai_trait_totalscore)[2], 2))
model_names <- c("Linear Regression", "Lasso", "Elastic Net", "Random Forest")

difference_rmse_TV1 <- round((as.numeric(RMSE_Test) - as.numeric(RMSE_Training)), 2)
difference_rsq_TV1 <- round((as.numeric(Rsq_Test) - as.numeric(Rsq_Training)), 2)

difference_rmse_TV2 <- round((as.numeric(RMSE_Test2) - as.numeric(RMSE_Training)), 2)
difference_rsq_TV2 <- round((as.numeric(Rsq_Test2) - as.numeric(Rsq_Training)), 2)

difference_rmse_V1V2 <- round((as.numeric(RMSE_Test2) - as.numeric(RMSE_Test1)), 2)
difference_rsq_V1V2 <- round((as.numeric(Rsq_Test2) - as.numeric(Rsq_Test1)), 2)

knitr::kable(data.frame(
    cbind(
    model_names,
    RMSE_Training,
    RMSE_Test1,
    difference_rmse_TV1,
    RMSE_Test2,
    difference_rmse_TV2,
    difference_rmse_V1V2,
    Rsq_Training,
    Rsq_Test1,
    difference_rsq_TV1,
    Rsq_Test2,
    difference_rsq_TV2,
    difference_rsq_V1V2)), 
    caption = 'Model and difference scores during train and tune: no self-report', row.names = FALSE)
```

### Model difference estimates and p-values, no self-report questionnaires
```{r Model difference comparison noself, echo=FALSE}
resamps <- resamples(
    list(
    LM = lmFit_kfold[[3]],
    LASSO = lassoFit_kfold[[3]],
    ENET = enetFit_kfold[[3]],
    RF = rfFit_kfold[[3]],
    LASSO_tune_noself = lassoFit_tune_noself,
    ENET_tune_noself = enetFit_tune_noself,
    RF_tune_noself = rfFit_tune_noself))
summary(diff(resamps))
```
```{r plot untuned and tuned model difference comparison noself, echo=FALSE}
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))
```

## Model performance comparison
```{r Second model comparison varying features, echo=FALSE, warning=FALSE}
RMSE_Training_all <- c(round(lmFit_kfold[[1]]$results[1,2], 2),
                       round(lassoFit_kfold[[1]]$results[1,2], 2),
                       round(enetFit_kfold[[1]]$results[1,3], 2),
                       round(rfFit_kfold[[1]]$results[1,2], 2))
Rsq_Training_all <-  c(round(lmFit_kfold[[1]]$results[1,3], 2),
                       round(lassoFit_kfold[[1]]$results[1,3], 2),
                       round(enetFit_kfold[[1]]$results[1,4], 2),
                       round(rfFit_kfold[[1]]$results[1,3], 2))

RMSE_Training_nodot <- c(round(lmFit_kfold[[2]]$results[1,2], 2),
                         round(lassoFit_kfold[[2]]$results[1,2], 2),
                         round(enetFit_kfold[[2]]$results[1,3], 2),
                         round(rfFit_kfold[[2]]$results[1,2], 2))
Rsq_Training_nodot <-  c(round(lmFit_kfold[[2]]$results[1,3], 2),
                         round(lassoFit_kfold[[2]]$results[1,3], 2),
                         round(enetFit_kfold[[2]]$results[1,4], 2),
                         round(rfFit_kfold[[2]]$results[1,3], 2))

RMSE_Training_noself <- c(round(lmFit_kfold[[3]]$results[1,2], 2),
                          round(lassoFit_kfold[[3]]$results[1,2], 2),
                          round(enetFit_kfold[[3]]$results[1,3], 2),
                          round(rfFit_kfold[[3]]$results[1,2], 2))
Rsq_Training_noself <-  c(round(lmFit_kfold[[3]]$results[1,3], 2),
                          round(lassoFit_kfold[[3]]$results[1,3], 2),
                          round(enetFit_kfold[[3]]$results[1,4], 2),
                          round(rfFit_kfold[[3]]$results[1,3], 2))

RMSE_Test_all <- c(NA, round(postResample(lasso_predict2, test_set$stai_trait_totalscore)[1], 2),
                   round(postResample(enet_predict2, test_set$stai_trait_totalscore)[1], 2),
                   round(postResample(rf_predict2, test_set$stai_trait_totalscore)[1], 2))
RMSE_Test_nodot <- c(NA, round(postResample(lasso_predict2_nodot, test_set$stai_trait_totalscore)[1], 2),
                     round(postResample(enet_predict2_nodot, test_set$stai_trait_totalscore)[1], 2),
                     round(postResample(rf_predict2_nodot, test_set$stai_trait_totalscore)[1], 2))
RMSE_Test_noself <- c(NA, round(postResample(lasso_predict2_noself, test_set$stai_trait_totalscore)[1], 2),
                      round(postResample(enet_predict2_noself, test_set$stai_trait_totalscore)[1], 2),
                      round(postResample(rf_predict2_noself, test_set$stai_trait_totalscore)[1], 2))

Rsq_Test_all <- c(NA, round(postResample(lasso_predict2, test_set$stai_trait_totalscore)[2], 2),
                  round(postResample(enet_predict2, test_set$stai_trait_totalscore)[2], 2),
                  round(postResample(rf_predict2, test_set$stai_trait_totalscore)[2], 2))
Rsq_Test_nodot <- c(NA, round(postResample(lasso_predict2_nodot, test_set$stai_trait_totalscore)[2], 2),
                    round(postResample(enet_predict2_nodot, test_set$stai_trait_totalscore)[2], 2),
                    round(postResample(rf_predict2_nodot, test_set$stai_trait_totalscore)[2], 2))
Rsq_Test_noself <- c(NA, round(postResample(lasso_predict2_noself, test_set$stai_trait_totalscore)[2], 2),
                     round(postResample(enet_predict2_noself, test_set$stai_trait_totalscore)[2], 2),
                     round(postResample(rf_predict2_noself, test_set$stai_trait_totalscore)[2], 2))
model_names <- c("Linear Regression", "Lasso", "Elastic Net", "Random Forest")

difference_rmse_AvND <- round((as.numeric(RMSE_Test_all) - as.numeric(RMSE_Test_nodot)), 2)
difference_rsq_AvND <- round((as.numeric(Rsq_Test_all) - as.numeric(Rsq_Test_nodot)), 2)

difference_rmse_AvNS <- round((as.numeric(RMSE_Test_all) - as.numeric(RMSE_Test_noself)), 2)
difference_rsq_AvNS <- round((as.numeric(Rsq_Test_all) - as.numeric(Rsq_Test_noself)), 2)

difference_rmse_NDvNS <- round((as.numeric(RMSE_Test_nodot) - as.numeric(RMSE_Test_noself)), 2)
difference_rsq_NDvNS <- round((as.numeric(Rsq_Test_nodot) - as.numeric(Rsq_Test_noself)), 2)

knitr::kable(data.frame(
    cbind(
    model_names,
    RMSE_Training_all,
    Rsq_Training_all,
    RMSE_Training_nodot,
    Rsq_Training_nodot,
    RMSE_Training_noself,
    Rsq_Training_noself,
    RMSE_Test_all,
    RMSE_Test_nodot,
    RMSE_Test_noself,
    Rsq_Test_all,
    Rsq_Test_nodot,
    Rsq_Test_noself)),
    caption = 'Summary model comparison varying included features', row.names = FALSE) 
```

## Pre-process holdout sample
### Zero and near zero variance
None of the variables exhibit zero or near-zero variance.  
```{r Calculate variance metrics holdout sample, include=FALSE}
# http://topepo.github.io/caret/pre-processing.html
# Frequency ratio & % of unique values
(nzv <- nearZeroVar(holdout_set, saveMetrics = TRUE))
```

### Linear dependencies
```{r Linear dependency holdout sample, include=FALSE}
# Identify linear combinations
(comboInfo <- findLinearCombos(holdout_set))
colnames(holdout_set[comboInfo$remove])

# Remove linear dependencies
holdout_set <- holdout_set[, -comboInfo$remove]
```
The following are linear combinations of other variables and are removed from the data set: `r colnames(holdout_set[comboInfo$remove])`  

### Scale the data
```{r scale data holdout}
ind <- sapply(holdout_set, is.numeric)
holdout_set[ind] <- lapply(holdout_set[ind], scale, scale = FALSE)
holdout_set[ind] <- lapply(holdout_set[ind], as.numeric)
```

## Final model testing on the hold-out testing sample
### Model with all features

For the final model, the Lasso algorithm is selected with a fraction of `r lassoFit_tune$finalModel$tuneValue$fraction`.
```{r Model performance testing on holdout, echo=FALSE}
predictions_all <- predict(lassoFit_tune, holdout_set)
```

#### Summary of model performance
["The function postResample can be used to estimate the root mean squared error (RMSE), simple R2, and the mean absolute error (MAE) for numeric outcomes."](http://topepo.github.io/caret/measuring-performance.html)
```{r summary stats all features, echo=FALSE}
summary_all <- postResample(predictions_all, holdout_set$stai_trait_totalscore)
```
```{r plot all feats v real, echo=FALSE}
plot(x = predictions_all,
     y = holdout_set$stai_trait_totalscore,
     main = 'Predicted Using All Features v. Real STAI Trait values')
```

### Model without dot probe
For the final model, the Lasso algorithm is selected with a fraction of `r lassoFit_tune_nodot$finalModel$tuneValue$fraction`.
```{r Model performance testing on holdout nodot, echo=FALSE}
predictions_nodot <- predict(lassoFit_tune_nodot, holdout_set)
```

#### Summary of model performance
```{r summary stats nodot, echo=FALSE}
(summary_nodot <-
     postResample(predictions_nodot, holdout_set$stai_trait_totalscore))
```
```{r plot predict nodot v real, echo=FALSE}
plot(x = predictions_nodot,
     y = holdout_set$stai_trait_totalscore,
     main = 'Predicted Without Dot Probe v. Real STAI Trait values')
```

### Model without self-report questionnaires
For the final model, the Lasso algorithm is selected with a fraction of `r lassoFit_tune_noself$finalModel$tuneValue$fraction`.
```{r Model performance testing on holdout noself, echo=FALSE}
predictions_noself <- predict(lassoFit_tune_noself, holdout_set)
```

#### Summary of model performance
```{r summary stats noself, echo=FALSE}
(summary_noself <-
     postResample(predictions_noself, holdout_set$stai_trait_totalscore))
```
```{r plot predict noself v real, echo=FALSE}
plot(x = predictions_noself,
     y = holdout_set$stai_trait_totalscore,
     main = 'Predicted Without Self-Report Qs v. Real STAI Trait values')
```

## Final model comparison 
```{r, echo=FALSE}
model_names <- c(
    'Lasso with all features',
    'Lasso without dot probe measures',
    'Lasso without self-report questionnaires')
RMSE <- c(round(summary_all[[1]], 2),
          round(summary_nodot[[1]], 2),
          round(summary_noself[[1]], 2))
Rsquared <- c(round(summary_all[[2]], 2),
              round(summary_nodot[[2]], 2),
              round(summary_noself[[2]], 2))
MAE <- c(round(summary_all[3], 2),
         round(summary_nodot[3], 2),
         round(summary_noself[3], 2))
knitr::kable(data.frame(cbind(model_names, RMSE, Rsquared, MAE)),
             caption = 'Summary of Model Predictions With and Without Dot Probe or Self-Report Measures',
             row.names = FALSE)
```

### Variable importance by model
#### Lasso model with all features
```{r Variable importance noself, echo=FALSE}
lassoImp_all <- varImp(lassoFit_tune, scale = FALSE)
plot(lassoImp_all, top = 20)
```

#### Lasso model without the dot probe
```{r, echo=FALSE}
lassoImp_nodot <- varImp(lassoFit_tune_nodot, scale = FALSE)
plot(lassoImp_nodot, top = 20)
```

# Conclusion
It is possible to predict trait anxiety scores as measured by the State-Trait Anxiety Inventory (STAI) with a high degree of accuracy using self-report measures. Using a lasso regularization regression model (fraction = `r lassoFit_tune$finalModel$tuneValue$fraction`), the prediction model performs well with self-report and demographic information. This model has an $R^{2}=`r round(summary_all[[2]], 2)`$ with an average deviation of predicted scores from real scores of $RMSE=`r round(summary_all[[1]], 2)`$ points on the STAI scale ($MAE=`r round(summary_all[[3]], 2)`$). Model fit is not improved significantly with inclusion of dot probe measures (fraction = `r lassoFit_tune_nodot$finalModel$tuneValue$fraction`; $RMSE=`r round(summary_nodot[[1]], 2)`; R^{2}=`r round(summary_nodot[[2]], 2)`; MAE=`r round(summary_nodot[[3]], 2)`$.  

None of the models performed  well without inclusion of the self-report questionnaire data. After tuning, however, the lasso model only has an $R^2`r round(summary_noself[[2]], 2)`$ and $MAE=`r round(summary_noself[[3]], 2)`$. We may therefore conclude that self-report measures are the driving features in the prediction of trait anxiety.  