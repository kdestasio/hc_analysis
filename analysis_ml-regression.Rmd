---
title: 'Hunter College Anxiety Dot-Probe: Machine Learning Regression Analysis'
author: "Krista DeStasio"
date: "11/06/2018"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: no
  pdf_document:
    latex_engine: xelatex
    toc: yes
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
# Clear workspace
rm(list = ls())

# Set paths and working directory
working_dir = '~/Dropbox/collaborations/hunter_college/hc_analysis/'
path_datafile = paste0(
    working_dir,
    '/data/N=844_FINAL_Traditional metrics_Trial Level metrics_questionnaires (n=837)_6.16.18.xlsx')
setwd(working_dir)

# Install and load required packages
list.of.packages <-
    c('janitor', 'caret', 'elasticnet', 'ranger', 'randomForest')
new.packages <-
    list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]
if (length(new.packages))
    install.packages(new.packages, repos = "http://cran.us.r-project.org")
lapply(list.of.packages, library, character.only = TRUE)

# Knit options
knitr::opts_chunk$set(echo = TRUE)
```

# Data
The data are the first dot probe and the self-report questionnaires from the study pictured below.  

![Figure 1. Study Design](/Users/kristadestasio/Dropbox/collaborations/hunter_college/study_design)

## Dot Probe Measures 

- `rt_neutral_nt`: Average RT on all neutral trials  
- `rt_threat_nt`: Average RT on all threat trials  
- `rt_baseline`: Average RT on baseline trials where two neutral faces appear and there is no competition for attention between threat and neutral faces; These trials appear randomly throughout the task  
- `rt_outliers`: # of trials considered to be outliers based on the following criteria (RTs faster than 150 ms or slower than 2000 ms; any trial RT that was +/- 3 SD from the person’s mean RT for that trial type)  
- `mean_pos`: mean of positive trial-level threat bias scores  
- `mean_neg`: mean of negative trial-level threat bias scores      
- `peak_pos`: highest positive trial-level threat bias score
- `peak_neg`: highest negative trial-level threat bias score           
- `variability`: absolute value of the distance across all trial-level threat bias scores / number of pairs 

## Questionnaires

- **STAI**: The State-Trait Anxiety Inventory (STAI) is a measure consisted of 20 items assessing state and 20 items assessing trait anxiety.
* _Spielberger, C. D., Gorsuch, R. L., Lushene, R., Vagg, P. R., & Jacobs, G. A. (1983). Manual for the State-Trait Anxiety Inventory. Palo Alto, CA: Consulting Psychologists Press._  
- **BDI-II**: The Beck Depression Inventory-Second Edition (BDI-II) is a 21-item self-report inventory assessing the severity of depression over the past two week.  
* _Beck, A. T., Steer, R. A., & Brown, G. K. (1996). Beck Depression Inventory–II. San Antonio: Psychological Corporation._  
- **BAI**: The Beck Anxiety Inventory (BAI) is a 21-item self-report questionnaire measuring the general level of anxiety.  
* _Beck, A. T., Epstein, N., Brown, G., & Steer, R. A. (1988). An inventory for measuring clinical anxiety: Psychometric properties. Journal of Consulting and Clinical Psychology, 56(6), 893._  
- **MASQ**: The Mood and Anxiety Symptom Questionnaire (short version) is a 62-item questionnaire that measures anxious symptoms, anxious arousal, depressive symptoms, and anhedonic depression in the past week period.  
* _Watson, D., & Clark, L. A. (1991). The Mood and Anxiety Symptom Questionnaire. Unpublished manuscript, University of Iowa, Department of Psychology, Iowa City._  
- **PANAS**: The Positive and Negative Affect Schedule (PANAS) is a set of two 10-item scales that measure the current and general positive and negative affect.  
* _Watson, D., Clark, L. A., & Tellegen, A. (1988). Development and validation of brief measures of positive and negative affect: the PANAS scales. Journal of Personality and Social Psychology, 54(6), 1063._  

# Regression Analysis: Outcome measure of interest is stai_trait_totalscore  

**Analysis Plan: Supervised learning, regression analysis**

Since values of the outcome measure (total trait anxiety score on the State-Trait Anxiety Inventory) are known, we will use supervised learning. With a continuous outcome variable a regression model is appropriate.  

The data are to be split into a [training set, test set, and holdout set](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets). Will do an initial 80/20 split (development/holdout), then a subsequent 70/30 train/test split of the initial (80%) development set.  

The goal of this analysis is to identify those features that are useful for predicting anxiety scores (trait) as measured by the State-Trait Anxiety Inventory (STAI).  

In addition to testing that a model can be created to predict anxiety scores, we will conduct training on data frames of (1) all the predictors, (2) a subset without the dot probe measures, and (3) a subset without self-report measures. This will help us determine whether inclusion of the dot-probe and/or self-report questionnaires provides significant added predictive power. The best fitting model for each of these 3 data sets will be applied to the holdout sample. We will examine the final model to identify which predictors are most informative.  

## Steps, outline

1. Split the data into train/test sets and hold-out sets. Will do an initial 80/20 split (development/holdout), then a subsequent 70/30 train/test split of the initial (80%) development set.  
2. Data cleaning/pre-processing
3. Initial model specification  
- Select model parameters. (i.e., coefficients in linear regression)  
- Baseline STAI anxiety score    
- Baseline dot probe  
- Select algorithms  
- linear regression  
- Lasso  
- Elastic net  
- Random forest  
- Cross-validation  
- k-fold cross validation with 10 folds  
4. Model testing on training set  
5. Model testing on test set
6. Adjust model parameters as needed - set hyperparameters. (repeat steps 3-5 until satisfied).  
7. Test final model on the hold-out sample.  
8. Evaluate model performance.  

## Data Cleaning
```{r Read in the data, include=FALSE}
dataframe_session1 <-
    clean_names(readxl::read_excel(path_datafile, sheet = 1, col_names = TRUE))
dataframe_questionnaires <-
    clean_names(readxl::read_excel(
        path_datafile,
        sheet = 2,
        col_names = TRUE,
        skip = 1))
dataframe_tlmetrics <-
    clean_names(readxl::read_excel(path_datafile, sheet = 1, col_names = TRUE))
```
```{r Check the data, include=FALSE}
str(dataframe_session1)
str(dataframe_questionnaires)
str(dataframe_tlmetrics)

table(c(
    colnames(dataframe_session1),
    colnames(dataframe_questionnaires),
    colnames(dataframe_tlmetrics)))
```

### Three variables are dropped from this data set

1. `session number` - all data are from a single session, making this variable irrelevant.  
2. `date of birth` - age is also a variable in this data set. DOB is ommitted as it would be perfectly correlated with age, which is retained.  
3. `subject id` - this is an identification tag, not a meaningful variable.  

```{r Create full data frame, include=FALSE}
abmt_data <- clean_names(readxl::read_excel(path_datafile))
# Drop the session and date of birth columns
abmt_data <-
    subset(abmt_data, select = -c(session, dob, subject_id))
# Display number of observations & features
dim(abmt_data)
```

### Missing data  
Machine learning can be used only on data sets without missing values. Rows with missing values are therefore omitted.  
```{r Drop missing values, include=FALSE}
abmt_data[abmt_data == 9999] <- NA
abmt_data[abmt_data == '#NULL!'] <- NA
missing_values = abmt_data[!complete.cases(abmt_data),]
missing_values[, 1] # These participants are missing data and are ommitted from the analyses
abmt_data = na.omit(abmt_data)
```
```{r Assign the variable types, include=FALSE}
abmt_data$gender <- as.factor(abmt_data$gender)
# Create dummy variables
abmt_data$gender <-
    model.matrix(stai_trait_totalscore ~ ., data = abmt_data[, 1:length(abmt_data)])[, 17]
```

## Data split 1
### 80/20 split (development/holdout)  
The data are split into two initial subsets. 80% of the full data are used for model training and testing 20% of the data form the holdout sample. The holdout sample is set aside until we decide on a final model. Once training and testing are complete and a final model selected, model performance is assesed by applying it to the holdout test set.  

```{r Split the data into holdout based on outcome, echo=FALSE}
set.seed(50)  # set.seed is a random number generator; the value in parentheses is arbitrary, and a seed is only set so that we can reproduce these same results next time we run the analysis.

# Split the original dataset into a training set and a testing set
partition_data <-
    createDataPartition(
        abmt_data$stai_trait_totalscore,
        times = 1,
        p = .8,
        list = FALSE)

development_set <- abmt_data[partition_data, ] # development set
holdout_set <- abmt_data[-partition_data, ] # holdout set
```

## Pre-Processing (development set)
### Zero and near zero variance
None of the variables exhibit zero or near-zero variance.  
```{r Calculate variance metrics, include=FALSE}
# http://topepo.github.io/caret/pre-processing.html
# Frequency ratio & % of unique values
(nzv = nearZeroVar(development_set, saveMetrics = TRUE))
```

### Linear dependencies
```{r Linear dependency, include=FALSE}
# Identify linear combinations
(comboInfo = findLinearCombos(development_set))
colnames(development_set[comboInfo$remove])

# Remove linear dependencies
development_set <- development_set[, -comboInfo$remove]
```
The following are linear combinations of other variables and are removed from the data set: `r colnames(development_set[comboInfo$remove])`  

## Data split 2
### 70/30 train/test split of the initial (80%) development set  
Three training data sets are created for the development of 3 different models. They are split as follows:    

```{r Split the data based on the outcome, results="hide"}
# See http://topepo.github.io/caret/data-splitting.html for more on splitting data
set.seed(50)
partition_data <-
    createDataPartition(
        development_set$stai_trait_totalscore,
        times = 1,
        p = .7,
        list = FALSE)

training_set <- as.data.frame(development_set[partition_data,])
test_set <- as.data.frame(development_set[-partition_data,])

# Without dot prob variables
training_set_nodot <-
    as.data.frame(development_set[partition_data, 13:length(development_set)])

# Without self-report questionnaire variables
training_set_noself <-
    as.data.frame(development_set[partition_data, c(1:14, 16)])
```

# Scale the training and testing data sets
```{r}
ind <- sapply(training_set_nodot, is.numeric)
training_set_nodot[ind] <- lapply(training_set_nodot[ind], scale)
training_set_nodot[ind] <- lapply(training_set_nodot[ind], as.numeric)

ind <- sapply(training_set_noself, is.numeric)
training_set_noself[ind] <- lapply(training_set_noself[ind], scale)
training_set_noself[ind] <- lapply(training_set_noself[ind], as.numeric)

ind <- sapply(test_set, is.numeric)
test_set[ind] <- lapply(test_set[ind], scale)
test_set[ind] <- lapply(test_set[ind], as.numeric)
```

## Data visualization 
### Rank of matrices

["A matrix is said to have full rank if its rank is either equal to its number of columns or to its number of rows (or to both)."](https://stackoverflow.com/questions/26558631/predict-lm-in-a-loop-warning-prediction-from-a-rank-deficient-fit-may-be-mis)  

```{r Matrix rank, include=FALSE}
# Check the rank of the data matrices
library(Matrix)
cat(rankMatrix(training_set), "\n")
cat(rankMatrix(test_set), "\n")
cat(rankMatrix(training_set_nodot), "\n")
cat(rankMatrix(training_set_noself), "\n")
```

**Full feature data set**: Both the training and test sets achieve full rank with a rank of `r rankMatrix(training_set)[1]`.  

**Data set without dot probe**: The training set achieves full rank with a rank of `r rankMatrix(training_set_nodot)[1]`.  

**Data set without self-report measures**: The training set achieves full rank with a rank of`r rankMatrix(training_set_noself)[1]`.  

### Data set description

**# of Observations, # of Features**

**Training set all predictors:** `r dim(training_set)`; train the model & tune parameters on this sample  
**Training set no dot probe:** `r dim(training_set_nodot)`; train the model & tune parameters on this sample   
**Training set no self-report questionnaires:** `r dim(training_set_noself)`; train the model & tune parameters on this sample  
**Testing set:** `r dim(test_set)`; initial model test and adjustment on this sample  
**Holdout set:** `r dim(holdout_set)`; hold-out sample, final test of model  

#### Data structure
```{r Look at the data, echo=FALSE}
str(development_set)
```

### Feature plots of each predictor with the outcome measure (stai_trait)
```{r Data visualization, echo=FALSE}
# Assign the regression features to an object
regVar <- colnames(development_set[,c(1:ncol(development_set))])

theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.4, .4, .4, .8)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)

featurePlot(x = development_set[, regVar][1:3], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][4:6], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][7:9], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][10:12], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][13:15], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][16:18], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][19:21], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][22:24], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
featurePlot(x = development_set[, regVar][25:26], 
            y = development_set$stai_trait_totalscore, 
            plot = "scatter",
            layout = c(3,1))
```

